{"cells":[{"cell_type":"markdown","metadata":{"id":"JDHEkf-bdriM"},"source":["# SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOOq7zs6dtsA"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.svm import LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","import time\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","def load_glove_model(glove_file):\n","    model = {}\n","    with open(glove_file, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            parts = line.split()\n","            word = parts[0]\n","            vector = np.array(parts[1:], dtype=np.float32)\n","            model[word] = vector\n","    return model\n","\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalnum() and token.lower() not in stop_words]\n","\n","    return ' '.join(tokens)\n","\n","def document_vector(doc, model, num_features):\n","    doc_vector = np.zeros((num_features,), dtype=\"float32\")\n","    num_words = 0\n","\n","    for word in doc.split():\n","        if word in model:\n","            num_words += 1\n","            doc_vector = np.add(doc_vector, model[word])\n","\n","    if num_words != 0:\n","        doc_vector = np.divide(doc_vector, num_words)\n","\n","    return doc_vector\n","\n","def train_and_predict(train_file, test_file, output_file, glove_file, num_features=300):\n","    glove_model = load_glove_model(glove_file)\n","\n","    train_data = pd.read_csv(train_file)\n","    train_data['processed_text'] = (train_data['title'] + \" \" + train_data['plot_synopsis']).apply(preprocess_text)\n","    train_document_vectors = np.array([document_vector(doc, glove_model, num_features) for doc in train_data['processed_text']])\n","    train_label = train_data[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']].values\n","\n","    test_data = pd.read_csv(test_file)\n","    test_data['processed_text'] = (test_data['title'] + \" \" + test_data['plot_synopsis']).apply(preprocess_text)\n","    test_document_vectors = np.array([document_vector(doc, glove_model, num_features) for doc in test_data['processed_text']])\n","\n","    svm_classifier = OneVsRestClassifier(LinearSVC(C=3), n_jobs=-1)\n","    start_training_time = time.time()\n","    svm_classifier.fit(train_document_vectors, train_label)\n","    end_training_time = time.time()\n","    print(\"Training time: \" + str(end_training_time - start_training_time))\n","\n","    start_testing_time = time.time()\n","    test_label = svm_classifier.predict(test_document_vectors)\n","    end_testing_time = time.time()\n","    print(\"Testing time: \" + str(end_testing_time - start_testing_time))\n","\n","    result_df = pd.DataFrame({'doc_id': test_data['ID']})\n","    result_df = pd.concat([result_df, pd.DataFrame(test_label)], axis=1)\n","    result_df.to_csv(output_file, index=False, header=False)\n","\n","glove_file = 'glove.6B.300d.txt'\n","train_and_predict('./data/Training-dataset.csv', './data/Task-2-test-dataset1.csv', '10728942-Task2-method-a.csv', glove_file)\n"]},{"cell_type":"markdown","metadata":{"id":"pCNlBh99dkRl"},"source":["# LSTM"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6204,"status":"ok","timestamp":1700840257552,"user":{"displayName":"Teh Deng Shyang","userId":"01332021879335042316"},"user_tz":0},"id":"-QqW7OMqto7y","outputId":"ccd98b58-336e-4ec7-ba12-a01ce23de7e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gK5dwHEuSpP","outputId":"c0565288-3ef4-470f-de0d-698d8eff08eb","executionInfo":{"status":"ok","timestamp":1700847631232,"user_tz":0,"elapsed":5263347,"user":{"displayName":"Teh Deng Shyang","userId":"01332021879335042316"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","Loading GloVe: 400001 vectors [00:33, 11998.49 vectors/s]\n","WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","207/207 [==============================] - 540s 2s/step - loss: 0.4604 - accuracy: 0.2569 - val_loss: 0.4295 - val_accuracy: 0.2930\n","Epoch 2/10\n","207/207 [==============================] - 508s 2s/step - loss: 0.4300 - accuracy: 0.2836 - val_loss: 0.4178 - val_accuracy: 0.3184\n","Epoch 3/10\n","207/207 [==============================] - 528s 3s/step - loss: 0.4108 - accuracy: 0.3175 - val_loss: 0.4059 - val_accuracy: 0.3154\n","Epoch 4/10\n","207/207 [==============================] - 512s 2s/step - loss: 0.3989 - accuracy: 0.3291 - val_loss: 0.3988 - val_accuracy: 0.3408\n","Epoch 5/10\n","207/207 [==============================] - 505s 2s/step - loss: 0.3871 - accuracy: 0.3487 - val_loss: 0.3964 - val_accuracy: 0.3469\n","Epoch 6/10\n","207/207 [==============================] - 508s 2s/step - loss: 0.3785 - accuracy: 0.3658 - val_loss: 0.3895 - val_accuracy: 0.3384\n","Epoch 7/10\n","207/207 [==============================] - 508s 2s/step - loss: 0.3686 - accuracy: 0.3791 - val_loss: 0.3892 - val_accuracy: 0.3311\n","Epoch 8/10\n","207/207 [==============================] - 508s 2s/step - loss: 0.3595 - accuracy: 0.3918 - val_loss: 0.3895 - val_accuracy: 0.3420\n","Epoch 9/10\n","207/207 [==============================] - 507s 2s/step - loss: 0.3530 - accuracy: 0.4011 - val_loss: 0.3926 - val_accuracy: 0.3384\n","Epoch 10/10\n","207/207 [==============================] - 502s 2s/step - loss: 0.3425 - accuracy: 0.4070 - val_loss: 0.3910 - val_accuracy: 0.3420\n","Training time: 5126.841902971268\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7913577af5b0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7913577af5b0> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","38/38 [==============================] - 5s 124ms/step\n","Testing time: 5.07581639289856\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Bidirectional, Dense, SpatialDropout1D\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import nltk\n","import string\n","from tqdm import tqdm\n","import time\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","def generate_output(model, X_test, output_file):\n","    start_testing_time = time.time()\n","    predictions = model.predict(X_test)\n","    end_testing_time = time.time()\n","    print(\"Testing time: \" + str(end_testing_time - start_testing_time))\n","\n","    binary_predictions = (predictions >= 0.5).astype(int)\n","\n","    output_df = pd.DataFrame(binary_predictions)\n","    output_df.insert(0, 'ID', test_ids)\n","    output_df.to_csv(output_file, index=False, header=False)\n","\n","def load_glove_embeddings(embeddings_path):\n","    embeddings_index = {}\n","    with open(embeddings_path, encoding='utf-8') as f:\n","        for line in tqdm(f, desc=\"Loading GloVe\", unit=\" vectors\"):\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","glove_path = 'glove.6B.300d.txt'\n","glove_embeddings_index = load_glove_embeddings(glove_path)\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def remove_stopwords_and_punctuation(text):\n","    stop_words = set(stopwords.words('english'))\n","    punctuation = set(string.punctuation)\n","\n","    word_tokens = word_tokenize(text)\n","    filtered_text = [word.lower() for word in word_tokens if word.lower() not in stop_words and word.lower() not in punctuation]\n","    filtered_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n","    return ' '.join(filtered_text)\n","\n","def create_embedding_matrix(tokenizer, embeddings_index, embedding_dim):\n","    word_index = tokenizer.word_index\n","    num_words = min(len(word_index) + 1, len(embeddings_index))\n","    embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","    for word, i in word_index.items():\n","        if i >= num_words:\n","            continue\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    return embedding_matrix\n","\n","def preprocess_data(file_path, tokenizer, max_len):\n","    df = pd.read_csv(file_path)\n","    documents = (df['title'] + ' ' + df['plot_synopsis']).apply(remove_stopwords_and_punctuation)\n","    sequences = tokenizer.texts_to_sequences(documents)\n","    X_test = pad_sequences(sequences, maxlen=max_len)\n","\n","    return df['ID'], X_test\n","\n","train_file_path = './data/Training-dataset.csv'\n","df_train = pd.read_csv(train_file_path)\n","df_train['processed_text'] = (df_train['title'] + ' ' + df_train['plot_synopsis']).apply(remove_stopwords_and_punctuation)\n","\n","max_words = 88151\n","max_len = 500\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(df_train['processed_text'])\n","sequences = tokenizer.texts_to_sequences(df_train['processed_text'])\n","X_train = pad_sequences(sequences, maxlen=max_len)\n","\n","embedding_dim = 300\n","embedding_matrix = create_embedding_matrix(tokenizer, glove_embeddings_index, embedding_dim)\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(9, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","batch_size = 32\n","epochs = 10\n","start_training_time = time.time()\n","model.fit(X_train, df_train[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']], epochs=epochs, batch_size=batch_size, validation_split=0.2)\n","end_training_time = time.time()\n","print(\"Training time: \" + str(end_training_time - start_training_time))\n","\n","test_file_path = './data/Task-2-validation-dataset.csv'\n","test_ids, X_test = preprocess_data(test_file_path, tokenizer, max_len)\n","\n","output_file = '10728942-Task2-method-b-validation.csv'\n","generate_output(model, X_test, output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"aSezMplCw9OW"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2888,"status":"ok","timestamp":1700847728665,"user":{"displayName":"Teh Deng Shyang","userId":"01332021879335042316"},"user_tz":0},"id":"0IrYJMacw_KO","outputId":"a5d9ba1d-97c4-4d2c-d4d6-e142d4760c29"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/NLP CW/Task 2\n","Class level: \n","Class  1 F1 score: 0.2601\n","Class  2 F1 score: 0.3869\n","Class  3 F1 score: 0.1873\n","Class  4 F1 score: 0.1379\n","Class  5 F1 score: 0.7472\n","Class  6 F1 score: 0.1463\n","Class  7 F1 score: 0.5992\n","Class  8 F1 score: 0.0000\n","Class  9 F1 score: 0.6501\n","----------------------------\n","Movie (document) level: \n","Precision: 0.5810\n","Recall: 0.5123\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/NLP CW/Task 2\n","\n","!python task2_eval_script_student_version.py '10728942-Task2-method-b-validation.csv' 'Task-2-validation-dataset.csv'"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
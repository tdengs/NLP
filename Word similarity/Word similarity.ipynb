{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1PqWy0EB_VY-xBug-CNBNpcEupf1bsuXh","authorship_tag":"ABX9TyOwGEWXbN9XwZt6ZD8rGOHD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Helper Functions"],"metadata":{"id":"TENf6LODRiqh"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n"],"metadata":{"id":"PuxNHiOQjZkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from numpy import dot\n","from numpy.linalg import norm\n","import pandas as pd\n","import csv\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","import time\n","\n","def cos_similarity(vector_a, vector_b):\n","    dot_product = np.dot(vector_a, vector_b)\n","    norm_a = np.linalg.norm(vector_a)\n","    norm_b = np.linalg.norm(vector_b)\n","\n","    similarity = dot_product / (norm_a * norm_b)\n","\n","    return similarity\n","\n","\n","def process_csv(model, input_file, output_file):\n","    start_time = time.time()\n","    with open(input_file, 'r', newline='') as infile, open(output_file, 'w', newline='') as outfile:\n","        reader = csv.reader(infile)\n","        writer = csv.writer(outfile)\n","\n","        for row in reader:\n","            # for validation data\n","            # id, word1, word2, gold_similarity = row\n","\n","            # for testing data\n","            id, word1, word2 = row\n","\n","            similarity = model(word1, word2)\n","\n","            writer.writerow([id, similarity])\n","\n","    end_time = time.time() - start_time\n","    print(end_time)\n","\n","\n","# Tokenize, remove stop words, lemmatize, and stem\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","def preprocess_text(text):\n","    words = text.lower().split()\n","    words = [word for word in words if word not in stop_words]\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    words = [stemmer.stem(word) for word in words]\n","    return words"],"metadata":{"id":"4q5-EDaNRlsH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Word2Vec"],"metadata":{"id":"9_KxehSj0T0m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4bEjzIvE5zy"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","df = pd.read_csv('./data/Training-dataset.csv')\n","\n","tokenized_synopses = [preprocess_text(text) for text in df['plot_synopsis']]\n","\n","# Train Word2Vec (using Skipgram)\n","word2vec_model = Word2Vec(sentences=tokenized_synopses, vector_size=300, window=1, min_count=1, workers=4, sg=1)\n","\n","# Special token for OOV words\n","oov_token = '<OOV>'\n","word2vec_model.wv[oov_token] = np.random.normal(size=word2vec_model.vector_size)\n","\n","def calculate_similarity_w2v(text1, text2):\n","    try:\n","        words1 = preprocess_text(text1)\n","        words2 = preprocess_text(text2)\n","\n","        # Get vectors for individual words or use the OOV token if not found\n","        vectors1 = [word2vec_model.wv[word] if word in word2vec_model.wv else word2vec_model.wv[oov_token] for word in words1]\n","        vectors2 = [word2vec_model.wv[word] if word in word2vec_model.wv else word2vec_model.wv[oov_token] for word in words2]\n","\n","        if not vectors1 or not vectors2:\n","            return 0.0\n","\n","        # Average the word vectors to obtain vectors for multi-word terms\n","        vector1 = np.mean(vectors1, axis=0)\n","        vector2 = np.mean(vectors2, axis=0)\n","\n","        return cos_similarity(vector1, vector2)\n","\n","    except KeyError:\n","        return 0.0\n","\n","input_file_path = './data/Task-1-test-dataset1.csv'\n","output_file_path = '/10728942-Task1-method-b.csv'\n","process_csv(calculate_similarity_w2v, input_file_path, output_file_path)\n"]},{"cell_type":"markdown","source":["# BERT"],"metadata":{"id":"ZyKAr1o9S7MO"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"nNLFi87nS8rl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import torch.nn.functional as F\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0]\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","def get_bert_embeddings(term, model, tokenizer):\n","    inputs = tokenizer(term, padding=True, truncation=True, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    embeddings = mean_pooling(outputs, inputs['attention_mask'])\n","    embeddings = F.normalize(embeddings, p=2, dim=1)\n","\n","    return embeddings\n","\n","def calculate_similarity_bert(text1, text2):\n","    try:\n","        embeddings1 = get_bert_embeddings(text1, model, tokenizer)\n","        embeddings2 = get_bert_embeddings(text2, model, tokenizer)\n","        # Convert tensors to numpy arrays for cosine_similarity\n","        embeddings1_np = embeddings1.cpu().numpy()\n","        embeddings2_np = embeddings2.cpu().numpy()\n","\n","        # Calculate cosine similarity using sklearn\n","        cos_sim = cosine_similarity(embeddings1_np, embeddings2_np)[0][0]\n","        return(cos_sim)\n","\n","    except Exception as e:\n","        print(f\"Error calculating similarity: {e}\")\n","        return 0.0\n","\n","if __name__ == \"__main__\":\n","    # https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n","    model_name = \"jinaai/jina-embeddings-v2-base-en\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    # trust_remote_code is needed to use the encode method\n","    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n","\n","    input_csv_file = './data/Task-1-test-dataset1.csv'\n","    output_csv_file = '10728942-Task1-method-c.csv'\n","\n","    process_csv(calculate_similarity_bert, input_csv_file, output_csv_file)\n","\n"],"metadata":{"id":"sZQEZQ3_0un_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Script"],"metadata":{"id":"-CeWpQ8uJEqO"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/NLP CW/Task 1\n","\n","\n","!python task1_eval_script_student_version.py '10728942-Task1-method-c-validation.csv' 'Task-1-validation-dataset.csv'"],"metadata":{"id":"mK16ZbgABmyD"},"execution_count":null,"outputs":[]}]}